{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709e8428-25da-40e6-865b-373ff2cc0d32",
   "metadata": {},
   "source": [
    "# Data Scraping from the Boston Marathon 2024 Website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4666dcb6-8f83-42d9-b302-26caaff68c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7065ab19-07ff-4276-b37d-1857ce213871",
   "metadata": {},
   "source": [
    "### Create a list of urls associated with each runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16b49ac3-b948-45ef-9f86-2a421ead5442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 8s\n",
      "Wall time: 29min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Each runner entry and data is on a separate url, this creates a list of all runner urls\n",
    "\n",
    "url_list = []\n",
    "\n",
    "for i in range(1,1175):\n",
    "    soup = scrape_race_data('https://results.baa.org/2024/?page='+ str(i) + '&event=R&event_main_group=runner&pid=search')\n",
    "    fullname_elements = soup.find_all(class_='list-field type-fullname')\n",
    "\n",
    "    # Extract and print all href attributes\n",
    "    for element in fullname_elements:\n",
    "        a_tag = element.find('a', href=True)\n",
    "        if a_tag:\n",
    "            url_list.append('https://results.baa.org/2024/' + str(a_tag['href']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d1fbbe-29c7-4f63-b391-e1a61f5fde62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29330"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verify that the number of urls matches the number of runner expected\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29b72dce-e945-49e3-9417-f2e54608373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the list of runner urls \n",
    "# with open('url_list.pkl', 'wb') as file:\n",
    "#     pickle.dump(url_list, file)\n",
    "\n",
    "#Load the list runner urls if perviously run and saved\n",
    "with open('url_list.pkl', 'rb') as file:\n",
    "    url_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6506efda-ec7f-4268-a51c-cb7b157c3d8f",
   "metadata": {},
   "source": [
    "### Extract data for each race participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8592ee-5990-46bd-9447-1d038b24d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary of all variables to be collected from each url\n",
    "\n",
    "marathon_dict = {\n",
    "    'name' : [], 'team' : [], 'age_group' : [], 'bib_num' : [], 'state' : [], 'place_mw' : [], 'place_ac' : [], \n",
    "    'place_total' : [], 'time_net' : [], 'time_gun' : [], 'mile_split' : [], 'race_status' : [], 'last_split' : [],\n",
    "    '5K_time_of_day' : [], '5K_time' : [], '5K_time_diff' : [], '5K_min_per_mile' : [],\n",
    "    '10K_time_of_day' : [], '10K_time' : [], '10K_time_diff' : [], '10K_min_per_mile' : [],\n",
    "    '15K_time_of_day' : [], '15K_time' : [], '15K_time_diff' : [], '15K_min_per_mile' : [],\n",
    "    '20K_time_of_day' : [], '20K_time' : [], '20K_time_diff' : [], '20K_min_per_mile' : [],\n",
    "    'HALF_time_of_day' : [], 'HALF_time' : [], 'HALF_time_diff' : [], 'HALF_min_per_mile' : [],\n",
    "    '25K_time_of_day' : [], '25K_time' : [], '25K_time_diff' : [], '25K_min_per_mile' : [],\n",
    "    '30K_time_of_day' : [], '30K_time' : [], '30K_time_diff' : [], '30K_min_per_mile' : [],\n",
    "    '20_miles_time_of_day' : [], '20_miles_time' : [], '20_miles_time_diff' : [], '20_miles_min_per_mile' : [],\n",
    "    '21_miles_time_of_day' : [], '21_miles_time' : [], '21_miles_time_diff' : [], '21_miles_min_per_mile' : [],\n",
    "    '35K_time_of_day' : [], '35K_time' : [], '35K_time_diff' : [], '35K_min_per_mile' : [],\n",
    "    '23_miles_time_of_day' : [], '23_miles_time' : [], '23_miles_time_diff' : [], '23_miles_min_per_mile' : [],\n",
    "    '24_miles_time_of_day' : [], '24_miles_time' : [], '24_miles_time_diff' : [], '24_miles_min_per_mile' : [],\n",
    "    '40K_time_of_day' : [], '40K_time' : [], '40K_time_diff' : [], '40K_min_per_mile' : [],\n",
    "    '25_2_miles_time_of_day' : [], '25_2_miles_time' : [], '25_2_miles_time_diff' : [], '25_2_miles_min_per_mile' : [],\n",
    "    '26_2_miles_time_of_day' : [], '26_2_miles_time' : [], '26_2_miles_time_diff' : [], '26_2_miles_min_per_mile' : []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1d3c8b-2fcc-434b-aa56-a8fe49be7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionaries to map from html locations to the dictionary names for each variable collected\n",
    "\n",
    "participant_elements_dict = {\n",
    "    'name' : 'f-__fullname last',\n",
    "    'team' : 'f-team last',\n",
    "    'age_group' : 'f-age_class last', \n",
    "    'bib_num': 'f-start_no_text last',\n",
    "    'state' : 'f-state last',\n",
    "    'place_mw' : 'f-place_all last',\n",
    "    'place_ac' : 'f-place_age last',\n",
    "    'place_total' : 'f-place_nosex last',\n",
    "    'time_net' : 'f-time_finish_netto last',\n",
    "    'time_gun' : 'f-time_finish_brutto last',\n",
    "    'mile_split' : 'f-total_min_mile last',\n",
    "    'race_status' : 'f-race_status last' ,\n",
    "    'last_split' : 'f-__last_split_name last' \n",
    "}\n",
    "\n",
    "race_elements_dict = {\n",
    "    '5K' : 'f-time_01 split',\n",
    "    '10K' : 'list-highlight f-time_02 split',\n",
    "    '15K' : 'f-time_03 split',\n",
    "    '20K' : 'list-highlight f-time_04 split',\n",
    "    'HALF' : 'f-time_05 split',\n",
    "    '25K' : 'list-highlight f-time_06 split',\n",
    "    '30K' : 'f-time_07 split',\n",
    "    '20_miles' : 'list-highlight f-time_42 split',\n",
    "    '21_miles' : 'f-time_43 split',\n",
    "    '35K' : 'list-highlight f-time_08 split',\n",
    "    '23_miles' : 'f-time_49 split',\n",
    "    '24_miles' : 'list-highlight f-time_50 split',\n",
    "    '40K' : 'f-time_09 split',\n",
    "    '25_2_miles' : 'list-highlight f-time_44 split',\n",
    "    '26_2_miles' : 'f-time_finish_netto highlight split'\n",
    "}\n",
    "\n",
    "race_subelements_dict = {\n",
    "    'time_of_day' : 'time_day',\n",
    "    'time' : 'time',\n",
    "    'time_diff' : 'diff right',\n",
    "    'min_per_mile' : 'min_km right opt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "185e3493-8766-4426-a0f9-4bafdf34fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions created to facilitate repetitive data collection tasks\n",
    "\n",
    "## Participant data is located with distinct htmp classes\n",
    "def get_participant_data(class_name, soup):\n",
    "    data = soup.find('td', class_=class_name)\n",
    "    if data is not None:\n",
    "        participant_data = data.get_text()\n",
    "        return participant_data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "## Race performance data is located within subclasses and must be extracted separately\n",
    "def get_race_data(class_name, subclass_name, soup):\n",
    "    data = soup.find('tr', class_= class_name)\n",
    "    if data is not None:\n",
    "        sub_data = data.find('td', class_= subclass_name)\n",
    "        if sub_data is not None:\n",
    "            race_data = sub_data.get_text()\n",
    "            return race_data\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "## Collection the html from each url\n",
    "def scrape_race_data(url):\n",
    "    # Make a GET request to the provided URL, make sure that there are no errors.\n",
    "    try:                                 \n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        return None\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parse HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c427e8b-c894-43cd-8d7e-6a4c75384e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Collect the data from each url using the provided functions\n",
    "for url in url_list:\n",
    "    html_data = scrape_race_data(url)\n",
    "    if html_data is not None:\n",
    "        # for element in participant_elements_dict: \n",
    "        #     class_name = participant_elements_dict.get(element)\n",
    "        #     participant_data = get_participant_data(class_name, html_data)\n",
    "        #     marathon_dict[element].append(participant_data)\n",
    "\n",
    "        for element_race in race_elements_dict:\n",
    "            race_class_name = race_elements_dict.get(element_race)\n",
    "            for subelement_race in race_subelements_dict:\n",
    "                race_subclass_name = race_subelements_dict.get(subelement_race)\n",
    "                race_data = get_race_data(race_class_name, race_subclass_name, html_data)\n",
    "                marathon_dict[element_race+ '_' + subelement_race].append(race_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1187afbc-435c-4368-8fea-7b355f808816",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save the dictionary of runner data\n",
    "# with open('marathon_dict.pkl', 'wb') as file:\n",
    "#     pickle.dump(marathon_dict, file)\n",
    "\n",
    "## Extract the dictionary of runner data\n",
    "with open('marathon_dict.pkl', 'rb') as file:\n",
    "    marathon_dict = pickle.load(file)\n",
    "\n",
    "## Convert data to a Dataframe\n",
    "marathon_data = pd.DataFrame.from_dict(marathon_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd6fe8-477f-4a6c-893e-dba93b7a5b83",
   "metadata": {},
   "source": [
    "### Extract Gender Data for each Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def9869-5934-4379-85c1-f45ae9432269",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The runner gender was provided on a separate webpage and had to be extracted separately and joined to the existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b107ea7a-ab5e-4846-b8c0-0ce77b127bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 16s\n",
      "Wall time: 13min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Extract the names of all female runners\n",
    "women_list = []\n",
    "\n",
    "for i in range(1,438):\n",
    "    soup = scrape_race_data('https://results.baa.org/2024/?page='+ str(i) + '&event=R&event_main_group=runner&pid=list&search%5Bsex%5D=W&search%5Bage_class%5D=%25')\n",
    "    fullname_elements = soup.find_all(class_='list-field type-fullname')\n",
    "\n",
    "    # Extract and print all href attributes\n",
    "    for name in fullname_elements:\n",
    "        #a_tag = element.find('a', href=True)\n",
    "        name_data = name.get_text()\n",
    "\n",
    "        if name_data:\n",
    "            women_list.append(str(name_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969d3384-fae5-4fc6-b70f-0542f8d8173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('women_list.pkl', 'wb') as file:\n",
    "    pickle.dump(women_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad5aef48-9322-46f9-86c6-9e89132e5982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 9s\n",
      "Wall time: 18min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Extract the names of all male runners\n",
    "men_list = []\n",
    "\n",
    "for i in range(1,585):\n",
    "    soup = scrape_race_data('https://results.baa.org/2024/?page='+ str(i) + '&event=R&event_main_group=runner&pid=list&search%5Bsex%5D=M&search%5Bage_class%5D=%25')\n",
    "    fullname_elements = soup.find_all(class_='list-field type-fullname')\n",
    "\n",
    "    # Extract and print all href attributes\n",
    "    for name in fullname_elements:\n",
    "        #a_tag = element.find('a', href=True)\n",
    "        name_data = name.get_text()\n",
    "\n",
    "        if name_data:\n",
    "            men_list.append(str(name_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de1b9d95-6120-4497-95fe-c78acb0ddba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('men_list.pkl', 'wb') as file:\n",
    "    pickle.dump(men_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29f3d888-5ec4-42d5-9c7e-9ea981484d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 828 ms\n",
      "Wall time: 4.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Extract the names of all non-binary runners\n",
    "bi_list = []\n",
    "\n",
    "for i in range(1,4):\n",
    "    soup = scrape_race_data('https://results.baa.org/2024/?page='+ str(i) + '&event=R&event_main_group=runner&pid=list&search%5Bsex%5D=X&search%5Bage_class%5D=%25')\n",
    "    fullname_elements = soup.find_all(class_='list-field type-fullname')\n",
    "\n",
    "    # Extract and print all href attributes\n",
    "    for name in fullname_elements:\n",
    "        #a_tag = element.find('a', href=True)\n",
    "        name_data = name.get_text()\n",
    "\n",
    "        if name_data:\n",
    "            bi_list.append(str(name_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fc19921-7dff-4e42-8e0e-2796e34311a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('binary_list.pkl', 'wb') as file:\n",
    "    pickle.dump(bi_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343a8fc-3858-4f5d-88ff-f3a89e327daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionary of all the gender data provided\n",
    "name_list = men_list+women_list+bi_list\n",
    "sex_list = (['M'] * len(men_list))+(['W'] * len(women_list))+(['X'] * len(bi_list))\n",
    "sex_dict = {\"name\":name_list, \"sex\":sex_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c95b1-a398-4949-b428-f9b9543bf2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a Dataframe \n",
    "sex_data = pd.DataFrame.from_dict(sex_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64e699e4-e162-4567-8039-c516fe561048",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the dataframe for future use\n",
    "with open('sex_data.pkl', 'wb') as file:\n",
    "    pickle.dump(sex_data, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
